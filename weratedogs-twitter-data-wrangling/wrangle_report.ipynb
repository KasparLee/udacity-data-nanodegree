{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangling Twitter Data for WeRateDogs - Wrangle Report\n",
    "###### By Kaspar Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, I will describe my efforts to wrangle the Twitter data of the WeRateDogs account.\n",
    "\n",
    "My work consisted of three stages:\n",
    "\n",
    "- Gathering Data\n",
    "- Assessing Data\n",
    "- Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this projected was gathered from the following sources:\n",
    "\n",
    "- **WeRateDogs Twitter Archive**: Downloaded from Udacity manually.\n",
    "- **Retweet and Favourite Counts**: For each tweet, with some additional data, was gathered using the Twitter API, storing the JSON data for each tweet in JSON data file `tweet_json.txt`\n",
    "- **Image Predictions**: This file was hosted by Udacity, and downloaded programatically using the `requests` library to save from the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all of the above data was gathered, I assessed them both visually and programmatically, for issues with both the quality and the tidiness of the data. I came across the following issues.\n",
    "\n",
    "#### Quality Issues:\n",
    "\n",
    "- 181 entries are retweets, and multiple unnecessary columns exist just for retweets\n",
    "- IDs are stored as integers or floats, rather than being stored as strings. They should not be stored as number types as they will should never be used in any sort of mathematical calculation.\n",
    "- Some tweets have no images (number of non-null `expanded_urls` is less than total number of entries)\n",
    "- Timestamps are stored as strings instead of more appropriate *datatime64* objects\n",
    "- Value in `source` column stored as HTML code rather than actual link\n",
    "- Rating numerator max value is 1776 (incorrectly extracted)\n",
    "- Rating denominator min value is 0 and max value is 170 (incorrectly extracted)\n",
    "- Name is incorrectly \"a\" for 55 entries, and \"None\" for 745 entries, should be a null value\n",
    "- Dog stage columns have \"None\" for empty values, a non-null value, for what should be a null value, such as `NaN`\n",
    "- `contributors`, `coordinates` and `geo` columns empty\n",
    "\n",
    "#### Tidiness Issues:\n",
    "\n",
    "- `df1` dog stage variable (i.e. doggo, floofer, pupper and puppo) should all be together in a single column, as they are all values for one variable, the type of dog.\n",
    "- Duplicate columns across dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the data, I had to resolve every single one of the issues found when assessing. I did not find any one-off issues that required manually cleaning.\n",
    "\n",
    "\n",
    "I utilised Pandas to programmatically clean and merge the datasets to produce one master dataset that is both clean and tidy.\n",
    "\n",
    "Each issue consisted of 3 cleaning stages; defining what I needed to do to clean the data, writing the code in order to clean the data, and testing that the data was clean after executing the code. Once the test displayed that the issue had been resolved, I moved onto the next issue, until all were resolved. \n",
    "\n",
    "This data was then used to create the analysis report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire data wrangling phase is very important into order to convert your raw, dirty, messy data into usable data that can be used to product useful visualisations. If the data was not cleaned, it would be very difficult to produce visualisations, and much of what would be produced would be somewhat inaccurate or misleading. Data wrangling allows us to eliminate this and produce useful visuals.\n",
    "\n",
    "Through these stages, I successfully wrangled the data in order to create a master dataset, that I then analysed to produce useful insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
